{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93e1122-2077-402f-af03-58ae2d23109b",
   "metadata": {},
   "source": [
    "**Implementing a two-layer perceptron.** In this coding session, we will get some practice implementing a two-layer perceptron. The code in this session is heavily borrowed from the [Machine Learning with PyTorch and Scikit-learn book's Github repo](https://github.com/rasbt/machine-learning-book/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3ded1-250f-4ac0-85a5-38877cca3409",
   "metadata": {},
   "source": [
    "**Notation for neural networks.** At the end of last class, we introduced the following notation for a neural network: let $(V, E, w)$ be a directed, weighted graph, and suppose the vertex set $V$ can be split into disjoint sets of vertices, $V = \\bigcup_{\\ell = 0}^L V_\\ell$ such that if $e \\in E$ then $e = (v, v')$ for $v \\in V_{\\ell}$ and $v' \\in V_{\\ell + 1}$ for some $\\ell = 0, 1, \\ldots, L - 1\\}$. The number $L$ is called the number of *layers* of the neural network. The integers $d_{\\rm in} := |V_0|$ and $d_{\\rm out} := |V_L|$ are called the *input* and *output* dimensions of the network. The parameter $w$ are the *weights*, which we can think of either as a function $w \\colon E \\to \\mathbb{R}$, or as a collection of matrices $(W_1, \\ldots, W_{L})$ where each $W_\\ell \\in \\mathbb{R}^{|V_{\\ell}| \\times |V_{\\ell - 1}|}$, along with biases $b_1, \\ldots, b_L$ such that $b_\\ell \\in \\mathbb{R}^{|V_{\\ell}|}$.\n",
    "\n",
    "We assume as given a non-linearity $\\sigma \\colon \\mathbb{R} \\to \\mathbb{R}$, for example a ReLU unit ($\\sigma(\\tau) := \\max(0, \\tau)$), a logistic unit $\\sigma(\\tau) = (1 + e^{-\\tau})$ or something else.\n",
    "\n",
    "The network then generates output via a *forward pass*. Given an input $x \\in \\mathbb{R}^{d_{\\rm in}}$, we calculate the output $o_0(x) = (x_1, \\ldots, x_d, 1)$, and then for subsequent layers we put $a_\\ell(x) = W_\\ell o_{\\ell - 1}(x) + b_\\ell$ and $o_{\\ell}(x) := \\sigma(a_{\\ell}(x))$ for output. The output of the network is, finally, $o_L(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41ad96-d2bd-460f-927a-f2754e7a8fc2",
   "metadata": {},
   "source": [
    "**Specializing to a two-layer perceptron.** In this lab, we will get into the nitty gritty details of a two-layer, fully connected, perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b789183-1b8d-4139-af69-f443b4f09f57",
   "metadata": {},
   "source": [
    "**Importing and preparing the MNIST dataset** We will use the MNIST dataset, which is a collection of images of handwritten digits (1, 2, ... , 9) half written by high school students and half written by employees of the US Census Bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0224e3-b209-46bd-9cb8-d8494523e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto')\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94179f06-9761-41fa-91b0-61b6b7c08c6a",
   "metadata": {},
   "source": [
    "The images in the MNIST dataset consist of 28×28 pixels, and each pixel is represented by a grayscale intensity value. Here, fetch_openml already unrolled the 28×28 pixels into one-dimensional row vectors, which are the rows in our X array (784 per row or image) above. The second array (y) returned by the fetch_openml function contains the corresponding target variable, the class labels (integers 0-9) of the handwritten digits.\n",
    "\n",
    "Next, let’s normalize the pixels values in MNIST to the range –1 to 1 (originally 0 to 255):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2fdd19-1eea-45ea-a870-fa26fa48a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X/255. - .5)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caf07a-7507-48cd-b918-cf9af8a65a25",
   "metadata": {},
   "source": [
    "We'll be using gradient descent based methods to train out model, and for these methods it is especially important that the features are normalized. Now let's visualize the first elements of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f66d32-d598-4537-b40d-8dfc18c3c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_4.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de945cd-b981-486a-ac49-6aee80fbd9a1",
   "metadata": {},
   "source": [
    "Now let's plot different 7s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5ba61-b8de-49ec-a4f8-0f51c85bd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X[y == 7][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/11_5.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd480c92-54fe-413b-9cbd-ebb621c0a66a",
   "metadata": {},
   "source": [
    "Finally, we will split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ad6f9-f994-48e7-8999-df8e92708fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the code into a train, validation, and test set of sizes\n",
    "# 55000, 5000, and 10000 respectively. Make sure you stratify on the\n",
    "# labels vector. (If you aren't sure how to do this, take a look at\n",
    "# the code from previous coding sessions.)\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931e485-5877-44e3-839d-a6354ccecce0",
   "metadata": {},
   "source": [
    "**Building a basic two-layer perceptron class.** We'll now work on implementing a basic two-layer neural network model. To this end, we'll need to handle a detail involving the encoding of the class labels. As they currently stand, the class labels are integers in $[9]$, but to work with them in a neural network model, we'll need to use an encoding to the output dimension. The encoding we'll use is known as a *one-hot* encoding, where the label $i \\in [9]$ is mapped to the $i + 1$st basis vector in $\\mathbb{R}^{10}$. The code for this encoding follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefebad-8020-46ff-bcfc-4bea21e6b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_onehot(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f298acf-dfcb-4a9b-91f3-44e2afea9507",
   "metadata": {},
   "source": [
    "We'll also use a sigmoid activation for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851477a-b558-4d6b-a5b1-55b757767d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the sigmoid method takes effect elementwise when applied to a numpy array\n",
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afafa8a-a298-4dc2-bb3e-49351017300e",
   "metadata": {},
   "source": [
    "We can now implement our 2 layer perceptron. Because it has two layers, its constructor will accept three integers, each corresponding to the number of nodes at each layer. An important detail to observe is that when we initialize the weights of our perceptron, we initialize them *randomly*. For now, we will ignore the details of the backpropogation which is necessary for training, and you will implement part of the initialization method as well as part of the forward propogation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d530c1-f0f1-42f6-81b5-03e9823cfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP:\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # hidden\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        # Initialize first weight matrix\n",
    "        self.weight_in = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_in = np.zeros(num_hidden)\n",
    "        \n",
    "        # Initialize second weight matrix in the same way, but with the right\n",
    "        # size\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=### YOUR CODE HERE ###)\n",
    "        self.bias_out = np.zeros(### YOUR CODE HERE###)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        a_h = np.dot(x, self.weight_in.T) + self.bias_in\n",
    "        o_h = sigmoid(a_h)\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        a_out = ### YOUR CODE HERE ###\n",
    "        o_out = ### YOUR CODE HERE ###\n",
    "        return o_h, o_out\n",
    "\n",
    "    def backward(self, x, o_h, o_out, y):  \n",
    "    \n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "        \n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        \n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_o_out = 2.*(o_out - y_onehot) / y.shape[0]\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_o_out__d_z_out = o_out * (1. - o_out) # sigmoid derivative\n",
    "\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_o_out * d_o_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # gradient for output weights\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = o_h\n",
    "        \n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "        \n",
    "\n",
    "        #################################        \n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__o_h = self.weight_out\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__o_h = np.dot(delta_out, d_z_out__o_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_o_h__d_z_h = o_h * (1. - o_h) # sigmoid derivative\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__o_h * d_o_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__o_h * d_o_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out, \n",
    "                d_loss__d_w_h, d_loss__d_b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d85c0-edd9-488b-a32f-510a79cd6d2b",
   "metadata": {},
   "source": [
    "Let's initialize our model; we'll make the hidden layer have 50 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56109e4-b852-477c-ab52-49eaf5de020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetMLP(num_features=28*28, num_hidden=50, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084901b2-c9ad-4a0e-9b65-cdb81a8d1647",
   "metadata": {},
   "source": [
    "**Reminders on GD, SGD, and mini-batch SGD.** Now, we'll a form of gradient descent to train out model. Here's a brief reminder on gradient descent methods (we'll go into more detail in lectures).\n",
    "\n",
    "Suppose we want to solve the minimization problem\n",
    "$$\n",
    "\\min_{\\theta \\in \\mathbb{R}^d} f(\\theta).\n",
    "$$ The function $f$ is called the *objective*. Then gradient descent uses the iteration, for some initial point $\\theta_0$,\n",
    "$$\n",
    "\\theta_{t + 1} = \\theta_t - \\eta_t \\nabla f(\\theta_t),\n",
    "$$ where $\\eta_t \\in \\mathbb{R}$ is a *learning rate* that is specified by the user (typically a constant in the case of gradient descent proper).\n",
    "\n",
    "An objective $f$ that arises frequently in statistics and machine learning is of the form\n",
    "$$\n",
    "f(\\theta) = \\frac1n \\sum_{i = 1}^n \\ell(h_{\\theta}(x_i), y_i),\n",
    "$$ where $\\ell$ is a loss function and $(h_{\\theta})_{\\theta \\in \\Theta}$ is a parametrized model class. Indeed, this objective corresponds to an *empirical loss*. For example, in the setting of two layer perceptrons we'll use $\\ell(z , y) := \\| z - y \\|^2$ the $\\ell_2$-loss, and $h_{\\theta}$ the neural network with parameters $\\theta = (W_1, W_2, b_1, b_2)$, and the sum will be over the training data of MNIST. Often (and in our case), the $n$ will be very large, making direct computation of $\\nabla f(\\theta)$ impractical and thus ruling out the gradient descent algorithm.\n",
    "\n",
    "A work-around for this issue, which has additional attractive properties that we won't discuss now, is called *stochastic gradient descent (SGD)*. Here, we take a uniformly random $i \\in [n]$, and use it to form a *stochastic gradient*\n",
    "$$\n",
    "\\hat \\nabla f(\\theta) := \\nabla_\\theta \\ell(h_{\\theta}(x_i), y_i).\n",
    "$$ Observe that, over the randomness of this uniform random sample, $\\mathbb{E}[\\hat \\nabla f(\\theta)] = \\nabla f(\\theta)$. It is thus plausible that we can use this stochastic gradient as a replacement for the full gradient $\\nabla f(\\theta)$, and run a stochastic version of the gradient descent algorithm. This is called *stochastic gradient descent*, and uses the updates\n",
    "$$\n",
    "\\theta_{t + 1} = \\theta_t - \\eta_t \\hat \\nabla f(\\theta_t),\n",
    "$$ where, importantly, we use fresh randomness at each iteration to form the stochastic gradient $\\hat \\nabla f(\\theta_t)$.\n",
    "\n",
    "Finally, the SGD algorithm can be excessively noisy due to the fact that we use only one data point to form the stochastic gradient $\\hat \\nabla f(\\theta_t)$. This is remedied by averaging over several stochastic gradients to produce *mini-batch SGD*. Specifically, let $M$ be a fixed positive integer (the mini-batch size) and let $\\{i_1, \\ldots, i_M\\} \\subset [n]$ be a random subset of $M$ integers in $[n]$. Then we can form the averaged stochastic gradient\n",
    "$$\n",
    "\\bar \\nabla^M f(\\theta) := \\frac1M \\sum_{m = 1 }^M \\nabla_\\theta \\ell(h_{\\theta}(x_{i_m}), y_{i_m})\n",
    "$$ The minibatch SGD algorithm is then\n",
    "$$\n",
    "\\theta_{t + 1} = \\theta_t - \\eta_t \\bar \\nabla^M f(\\theta_t).\n",
    "$$ In practice, the way the randomness for the minibatch data or SGD is typically generated is by first randomly permuting the training data, and then iterating through the permuted data until there is no more data left, and then re-permuting the data and starting over again. Each iteration through the training data is referred to as an *epoch*.\n",
    "\n",
    "The following utility function (no need to read carefully) generates minibatches appropriate for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a02df0-dc6d-43aa-ad66-b60e1acd0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279d422-8d7b-4b07-a4e9-c33adaf384a9",
   "metadata": {},
   "source": [
    "Let's now define functions that compute the MSE and accuracy of our model, and check that they give reasonable numbers when run on our existing model (remember, we haven't trained it yet so it is merely randomly initialized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7385f31-f151-4626-a5aa-4cab0edb3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f736a2-ee34-4f0a-af79-4ea4f3aeacca",
   "metadata": {},
   "source": [
    "This is a plausible accuracy (about 1/10) since we initialized the data randomly.\n",
    "\n",
    "Now, we'll use the minibatch method to implement a version of the above methods that will scale to very large data sets where we might otherwise run into memory issue. There's no need to look at this method in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506467b3-e75e-40cc-8434-abf095030607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/(i+1)\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e531c-e5c4-47b6-91ab-b08bb3607c11",
   "metadata": {},
   "source": [
    "We'll get the same result as before using this large-scale version of the previous methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd5785-5f84-4413-9e54-f75baa655f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a0469-fd4f-4c98-a01c-240eb72c0ed3",
   "metadata": {},
   "source": [
    "We can now, finally, implement the training method! Note that we will be keeping track of the train and validation MSE and accuracy throughout the optimization so we can make plots below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1d979-53a5-4eec-b783-39fa08a90d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            o_h, o_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Compute gradients ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, o_h, o_out, y_train_mini)\n",
    "\n",
    "            # Each of the d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h\n",
    "            # represent the gradient of the loss with respect to the parameters\n",
    "            # w_out, b_out, w_h, b_h, averaged over the minibatch\n",
    "            # use them to update the weights here usign the mini-batch SGD formula\n",
    "\n",
    "            #### Update weights ####\n",
    "            model.weight_in -= ### YOUR CODE HERE ###\n",
    "            model.bias_in -= ### YOUR CODE HERE ###\n",
    "            model.weight_out -= ### YOUR CODE HERE ###\n",
    "            model.bias_out -= ### YOUR CODE HERE ###\n",
    "        \n",
    "        #### Epoch Logging ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af69961-33b2-42b7-8bb3-22395502bfe7",
   "metadata": {},
   "source": [
    "Let's see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e09a6b-78f7-4776-9e72-e5549ba8c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0069d9-c026-493a-a5aa-a97d700949d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.savefig('figures/11_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccff41-1d3a-41d7-b5e3-12e99b8c575d",
   "metadata": {},
   "source": [
    "Now, create a plot of the Accuracy vs Epochs of both the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b76dfc-fd7c-40b8-a369-600cf5e0bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='lower right')\n",
    "#plt.savefig('figures/11_08.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e73ac-90dc-4eca-bb67-1c3f8eea508f",
   "metadata": {},
   "source": [
    "We finally look at the ultimate test accuracy of the trained model, and plot some of the misclassified digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f523f-d695-4504-8cf2-c23fda130a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9143b-1a45-4110-b570-21f36897f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "_, probas = model.forward(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_09.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
