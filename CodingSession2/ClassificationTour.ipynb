{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8f6b04-907c-47c0-943c-f671fc076be8",
   "metadata": {},
   "source": [
    "**Using scikit-learn for classification** In this coding session, we will use the scikit-learn framework to explore different classification algorithms. You are strongly encouraged to use the [scikit-learn docs](https://scikit-learn.org/stable/) as a reference during this session. Most of the code here was taken from the [Machine Learning with PyTorch and Scikit-learn book's Github repo](https://github.com/rasbt/machine-learning-book/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76492cf1-799f-4f3e-8801-98118b329a18",
   "metadata": {},
   "source": [
    "We'll start by loading in the Iris dataset once more. This time, we'll consider only the Petal length and Petal width features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b49f6-cef5-44b6-9661-0d05e166081d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[50:, [2, 3]]\n",
    "y = iris.target[50:]\n",
    "\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b820e-83ee-41f6-92fd-9e738b8e7999",
   "metadata": {},
   "source": [
    "**Best practice #1: train-test splits** One thing we didn't consider last time was the issue of splitting data into train and test sets. The idea here is that to get an idea of the generalization performance of a model, we can train on only a subset of the data and use the remaining data to assess generalization (more sophisticated versions of this idea split into three groups: train, valididate, and test). scikit-learn has convenient functionality for splitting data in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8f016-d071-4297-90e3-29557c83ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28695a88-4a75-4242-8b14-91a2f28dff10",
   "metadata": {},
   "source": [
    "A word on the parameters here: first, the test_size parameter controls the number of data points that are held out for the test set (in this case, 50%). The random_state parameter is the random seed used to randomly split the data, and ensures the code is reproducible. And finally, the stratify parameter ensures that the train and test sets are split equally between the classes. This last issue is a best practice that is extremely important, since in many applications one class is significantly more rare than another (e.g. medical diagnoses for rare conditions).\n",
    "\n",
    "We can check that the method did indeed split with the correct proportions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60f3bb-1099-4b3f-abaa-e3e56598de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db708be-a08c-4622-ba6b-6f6a04435ad0",
   "metadata": {},
   "source": [
    "**Best-practice #2: feature standarization.** Another important best-practice that we haven't talked about yet is that of feature normalization. This has various meanings, but one of the most important is that we transform the data so that it is mean zero and has identity covariance. The reason for this is that many algorithms (although not all) are sensitive to the *scale* of the data. For example, if we are doing linear regression and one of the covariates has naturally much larger domain than another (e.g. income vs. age) then it would influence the outcome of the regression to an excessive degree.\n",
    "Feature standarization means we take data $\\mathcal{X} = (x_1, \\ldots, x_n) \\in (\\mathbb{R}^d)^n$ and transform it to\n",
    "$$\n",
    "\\mathcal{X}_{\\mathrm{std}} = (Ax_1 + b, Ax_2 + b, \\ldots, Ax_n + b) \\in (\\mathbb{R}^d)^n\n",
    "$$ where $A \\in \\mathbb{R}^{d \\times d}$ and $b \\in \\mathbb{R}^d$ are such that the transformed\n",
    "data is **mean-zero** and **isotropic**, meaning that\n",
    "$$\n",
    "\\sum_{x_{\\mathrm{std}} \\in \\mathcal{X}_{\\mathrm{std}}} x_{\\mathrm{std}} = 0,\n",
    "$$ and\n",
    "$$\n",
    "\\sum_{x_{\\mathrm{std}} \\in \\mathcal{X}_{\\mathrm{std}}} x_{\\mathrm{std}} x_{\\mathrm{std}}^T = I_d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a0bbd-ab32-4261-b5d8-6c152b4a6929",
   "metadata": {},
   "source": [
    "The following code uses scikit-learn to accomplish this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402dfc0-d8ee-4f5f-8d05-46b2a6e6962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "# NOTICE: we apply this same transformation to the test data. This\n",
    "#       is a very important point that could be easy to forget!\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a3c69-9dd1-41fc-ad04-a5f6448e5424",
   "metadata": {},
   "source": [
    "Now that we have prepared our data using the above two best-practices, we can get around to fitting some models with scikit-learn!\n",
    "\n",
    "Let's start with the Perceptron. The following code initializes a Perceptron model and fits it to the training data. The careful reader will notice that we are running the Perceptron algorithm on data with more than $2$ classes. This is handled with a *one-vs.-all (OvA)* approach where we train one classifier for each class (which treats the class as positive examples and all others as negative) and then for new data points we choose the classifier with the highest confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d92469-ab18-410b-bab9-1eb635a39dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# The random_state here is a seed to shuffle the training data, which\n",
    "# we fix to make the output reproducible\n",
    "ppn = Perceptron(random_state=1)\n",
    "\n",
    "# This line fits the model with X_train_std as the feature data and\n",
    "# y_train as the response data\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2ca65-593c-426b-8095-d4f0bfe4d0ba",
   "metadata": {},
   "source": [
    "Notice the similarities between this call and the Perceptron class we implemented last time. Indeed, recall that the API that we implemented last time supported calls of the form `.fit(X, y)` as well as `.predict(X)`. The former fitted the model with input `X, y` and the latter computed predictions on new data `X`. This general structure is shared by much of the scikit-learn API, as we'll see repeatedly below.\n",
    "\n",
    "In particular, we can compute predictions as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2fce2-50a4-4f58-beab-855b5ced4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cc942-0c0c-44e7-9bb3-f28eb373871e",
   "metadata": {},
   "source": [
    "We'll now plot the decision regions that this results in, using similar code to last time (and again, there's no need to look at this code in detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4624aee-d2da-4ec3-b30e-c0a4a32f93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To check recent matplotlib compatibility\n",
    "import matplotlib\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = np.array(('red', 'blue', 'lightgreen', 'gray', 'cyan'))\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=f'Class {cl}', \n",
    "                    edgecolor='black')\n",
    "\n",
    "    # highlight test examples\n",
    "    if test_idx:\n",
    "        # plot all examples\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c='none',\n",
    "                    edgecolor='black',\n",
    "                    alpha=1.0,\n",
    "                    linewidth=1,\n",
    "                    marker='o',\n",
    "                    s=100, \n",
    "                    label='Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fbf59-7ecc-442d-b9bd-ce7f4cd07e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "plot_decision_regions(X=X_combined_std, y=y_combined,\n",
    "                      classifier=ppn, test_idx=range(50, 100))\n",
    "plt.xlabel('Petal length [standardized]')\n",
    "plt.ylabel('Petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/03_01.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7277a72-ddfd-430e-8ad8-78504b54c04f",
   "metadata": {},
   "source": [
    "We can also easily use an SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06dbc1-8f5d-42d3-8523-ffe84994dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "plot_decision_regions(X_combined_std, \n",
    "                      y_combined,\n",
    "                      classifier=svm, \n",
    "                      test_idx=range(50, 100))\n",
    "plt.xlabel('Petal length [standardized]')\n",
    "plt.ylabel('Petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/03_11.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25a75f-36c9-45a3-ae81-981016d7e988",
   "metadata": {},
   "source": [
    "The following code creates a kernelized SVM with kernel $K(x, x') := e^{-\\gamma \\|x - x'\\|^2}$, where $\\gamma > 0$ is a parameter that controls the width of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40f21c-59ac-4e0f-b9ff-f69dfd4caccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', random_state=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e50935-ac0e-4cd6-8207-bb3fc191479c",
   "metadata": {},
   "source": [
    "Now, fit this model and plot its performance on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb55d7-bc9b-4667-abab-134b9e1f1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc798e7a-6c35-4a5b-9390-4e4c2265ce4e",
   "metadata": {},
   "source": [
    "Let's play also with the $\\gamma$ parameter: create another SVM with a much larger $\\gamma$ parameter and plot the resulting decision regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c46fe43-a8d9-4f91-afcd-ddbf4deb9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac42861-9879-4f23-87bf-c26d3b147c0f",
   "metadata": {},
   "source": [
    "Intuitively, why are the decision regions like they are for large $\\gamma$? Do you think such a model would generalize well?\n",
    "\n",
    "Next, we'll try a decision tree classifier. Notice that decision trees are not sensitive to the scale of the data, so we don't need to use the standardized data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1254cd7-7879-455e-a78f-1327ac0fc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier(criterion='gini', \n",
    "                                    max_depth=4, \n",
    "                                    random_state=1)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X_combined, y_combined, \n",
    "                      classifier=tree_model,\n",
    "                      test_idx=range(50, 100))\n",
    "\n",
    "plt.xlabel('Petal length [cm]')\n",
    "plt.ylabel('Petal width [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7e6b7-ff11-43bb-a497-e341d8f2acfb",
   "metadata": {},
   "source": [
    "**Implementing AdaBoost with decision stumps.** Finally, we'll implement the AdaBoost algorithm with decision stumps. Recall that this algorithm assumes access to a weak learner that given any weight classification problem, returns a classifier that does slightly better than average.\n",
    "\n",
    "- **Input:** $X \\in \\mathcal{X}^n$, $y \\in \\{-1, 1\\}^n$, $T \\in \\mathbb{N}$\n",
    "- Initialize $\\mu^{(0)} = (1/n)_{i =1 }^n$\n",
    "- For $t = 1, \\ldots, T$:\n",
    "    - Let $h_t$ be a weak learner for the data $(X, y)$ with\n",
    "    weight vector $\\mu^{(t)}$\n",
    "    - Put $\\varepsilon_t := \\sum_{i = 1}^n \\mu_i^{(t)} 1[h_t(x_i) \\neq y_i]$\n",
    "    -  Put $w_t := \n",
    "        (1/2) \\log(1/\\varepsilon_t - 1)$.\n",
    "    - Set $\\mu^{(t + 1/2)}_i := \n",
    "        \\mu^{(t)}_i \\cdot\n",
    "        e^{-w_t y_i h_t(x_i)}$.\n",
    "    - Let $\\mu^{(t + 1)} := \\mu^{(t + 1/2)}/\\langle 1, \n",
    "        \\mu^{(t + 1/2)} \\rangle$.\n",
    "- Return the weighted ensemble classifier\n",
    "    $\n",
    " h_w \\colon x \\mapsto \n",
    "    \\mathrm{sgn}\\big( \\sum_{ t= 1}^T\n",
    "    w_t h_t(x) \\big).\n",
    "    $\n",
    "\n",
    "For the purposes of our implementation, you may assume that the labels $y$ are always in $\\{-1, 1\\}$ (when we apply AdaBoost to the Iris dataset below we'll transform the data so this is true).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12718474-35a8-4ac1-93ed-6b69b917a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def DecisionStumpLearner(X, y, mu):\n",
    "    stump = DecisionTreeClassifier(criterion='gini', \n",
    "                                    max_depth=1, \n",
    "                                    random_state=1)\n",
    "\n",
    "    stump.fit(X, y, mu)\n",
    "\n",
    "    return stump\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, T, weak_learner=DecisionStumpLearner):\n",
    "        self.T = T\n",
    "        self.weak_learner = weak_learner\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = np.shape(X)[0]\n",
    "        self.w = np.zeros(self.T)\n",
    "        \n",
    "        mu = np.ones(n)/n\n",
    "        self.learners = []\n",
    "\n",
    "        for t in range(0, self.T):\n",
    "           ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0, 1, -1)\n",
    "\n",
    "    def net_input(self, X):\n",
    "        input = np.zeros(np.shape(X)[0])\n",
    "        for t in range(0, self.T):\n",
    "            input += self.w[t]*self.learners[t].predict(X)\n",
    "\n",
    "        return input\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ed00c-c70a-404a-b25e-97492e547159",
   "metadata": {},
   "source": [
    "Let's use this AdaBoost implementation and see what the decision regions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74ae3e-3f3b-400b-a4ff-7eafec202cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf = AdaBoost(20)\n",
    "\n",
    "# let's first map label 2 to +1 and label 1 to -1 so\n",
    "# we may apply our AdaBoost implementation\n",
    "y_train_modified = np.where(y_train >= 1.5, 1, -1)\n",
    "y_test_modified = np.where(y_test >= 1.5, 1, -1)\n",
    "\n",
    "# now fit adaboost_clf and plot its decision regions\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec0470-d56b-4f9e-85ab-afd77c1f3e15",
   "metadata": {},
   "source": [
    "Notice that AdaBoost can easily fit the training data to 0 error, which should give you some cause for concern. Indeed, AdaBoost is known to suffer from overfitting and so a conservative number of iterations, which may be selected by cross-validation, are recommended."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
